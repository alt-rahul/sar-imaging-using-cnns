{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_dir_2, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_dir_2 = image_dir_2\n",
    "        self.image_files = [i for i in os.listdir(image_dir) if i.endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        self.images_files_2 = [i for i in os.listdir(image_dir_2) if i.endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        self.transform = transform\n",
    " \n",
    "    def len(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def getItem(self, dirc, files, idx):\n",
    "        img_path = os.path.join(dirc, files[idx])\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  \n",
    "    \n",
    "    def getDataset(self, limit):\n",
    "        total_dataset = []\n",
    "        for i in range(1, limit):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Have reached {i}/{len(self.image_files)}\")\n",
    "            total_dataset.append([self.getItem(dirc =self.image_dir, files=self.image_files, idx=i), self.getItem(dirc =self.image_dir_2, files=self.images_files_2, idx=i)])\n",
    "        return total_dataset\n",
    "    \n",
    "    def getImage(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image_1 = Image.open(img_path).convert(\"L\")\n",
    "        img_path_2 = os.path.join(self.image_dir_2, self.images_files_2[idx])\n",
    "        image_2 = Image.open(img_path_2).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image_1 = self.transform(image_1)\n",
    "            image_2 = self.transform(image_2)\n",
    "        return [image_1, image_2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "clean_dir = os.environ[\"DATA_DIR\"]\n",
    "noise_dir = os.environ['DATA_DIR_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = Compose([\n",
    "    ToTensor(), \n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(image_dir=clean_dir, image_dir_2=noise_dir, transform=transformations).getDataset(301)\n",
    "test_dataset = ImageDataset(image_dir=clean_dir, image_dir_2=noise_dir, transform=transformations).getDataset(201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_batches, train_target_batches = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_batches.shape, train_target_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rand = torch.randint(low=1, high=BATCH_SIZE, size=(1,)).item()\n",
    "test_image = train_image_batches[rand]\n",
    "test_target = train_target_batches[rand]\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.subplot(3,3, 1)\n",
    "plt.imshow(test_image.squeeze(), cmap='gray')\n",
    "plt.title('clean')\n",
    "plt.axis(False)\n",
    "plt.subplot(3,3,2)\n",
    "plt.imshow(test_target.squeeze(), cmap='gray')\n",
    "plt.title('noisy')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cell that creates the model so make sure you run this like \n",
    "# for real, this is no joke\n",
    "\n",
    "class SARDespeckleModel(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.initial_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.middle_stack = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(num_features=hidden_units),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for _ in range(6)\n",
    "            ]\n",
    "        )\n",
    "        self.final_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=output_shape, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "    def forward(self, initial:torch.Tensor):\n",
    "        x = self.initial_stack(initial)\n",
    "        x = self.middle_stack(x)\n",
    "        res = self.final_stack(x)\n",
    "        despeckle = initial/(res + 1e-8)\n",
    "        img = self.tanh_layer(despeckle)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = SARDespeckleModel(\n",
    "    input_shape=1,\n",
    "    hidden_units=64,\n",
    "    output_shape=1,\n",
    ")\n",
    "\n",
    "# ks = [32, 64, 128]\n",
    "# models = [[SARDespeckleModel(1, 64, 1) for x in range(256**2 / k**2)] for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(img, weight):\n",
    "    bs_img, c_img, h_img, w_img = img.size()\n",
    "    tv_h = torch.pow(img[:,:,1:,:]-img[:,:,:-1,:], 2).sum()\n",
    "    tv_w = torch.pow(img[:,:,:,1:]-img[:,:,:,:-1], 2).sum()\n",
    "    return weight*(tv_h+tv_w)/(bs_img*c_img*h_img*w_img)\n",
    "\n",
    "def loss_fn(predicted, target):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    L_E = mse_loss(predicted, target) \n",
    "    L_TV = total_variation_loss(predicted, .002)\n",
    "    loss = L_E +  L_TV\n",
    "    return loss\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model0.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    total_loss =0\n",
    "    model0.to(device)\n",
    "    model0.train()\n",
    "    for batch, (clean_img, noise_img) in enumerate(train_dataloader):\n",
    "\n",
    "        clean_img, noise_img = clean_img.to(device), noise_img.to(device)\n",
    "        img_pred = model0(noise_img)\n",
    "        loss = loss_fn(img_pred, clean_img)\n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch % 400 ==0:\n",
    "            print(f\"We have passed {batch * len(clean_img)}/{len(train_dataloader.dataset)} samples\")\n",
    "            print(f'Current Accumalting Total Loss: {total_loss:.6f} | Current Loss: {loss} \\n---------')\n",
    "    \n",
    "    total_loss /= len(train_dataloader)\n",
    "    print(f\"So far our train loss has been: {total_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.eval()\n",
    "test_loss = 0\n",
    "before_int = 0\n",
    "test_img_pred = 0\n",
    "test_img_label = 0\n",
    "with torch.inference_mode():\n",
    "    for (y, X) in test_dataloader:\n",
    "        y, X = y.to(device), X.to(device)\n",
    "        img_pred = model0(X)\n",
    "        test_loss += loss_fn(img_pred, y)\n",
    "        before_int = X\n",
    "        test_img_pred = img_pred\n",
    "        test_img_label = y\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "\n",
    "print(f'Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_pred_sample, test_img_label_sample = test_img_pred[0].to('cpu'), test_img_label[0].to('cpu')\n",
    "original = before_int[0].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "def plot_cm(test, pred, og):\n",
    "    plt.subplot(2,2, 1)\n",
    "    plt.imshow(test.squeeze(), cmap='gray')\n",
    "    plt.title('prediction')\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(pred.squeeze(), cmap='gray')\n",
    "    plt.title('target')\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(og.squeeze(), cmap='gray')\n",
    "    plt.title('original')\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "\n",
    "plot_cm(test_img_pred_sample, test_img_label_sample, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "psnr = PeakSignalNoiseRatio()\n",
    "ssim = StructuralSimilarityIndexMeasure()\n",
    "\n",
    "print(f\"PSRN (higher the better): {psnr(test_img_pred_sample, test_img_label_sample)}\") \n",
    "print(f\"SSIM (0-1 scale): {ssim(test_img_pred.to('cpu'), test_img_label.to('cpu'))}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = 'sar_model'\n",
    "MODEL_SAVE_PATH = MODEL_PATH/ MODEL_NAME   \n",
    "\n",
    "print(f'saving model to {MODEL_SAVE_PATH}')\n",
    "torch.save(obj=model0.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.eval()\n",
    "sample_dataset = train_dataset = ImageDataset(image_dir=clean_dir, image_dir_2=noise_dir, transform=transformations)\n",
    "pred_imgs = []\n",
    "with torch.inference_mode():\n",
    "    for i in range(69, 73):\n",
    "        original, noise_sample = sample_dataset.getImage(idx=i)\n",
    "        input_img = noise_sample.to(device).unsqueeze(dim=0)\n",
    "        pred = model0(input_img)\n",
    "        pred = pred[0].to('cpu')\n",
    "        plot_cm(pred, original, noise_sample)\n",
    "        print(f\"Prediction Image PSNR (higher the better): {psnr(pred, original):.3f} vs Original Noisy PSRN: {psnr(noise_sample, original):.3f}\") \n",
    "        print(f\"Predicction Image SSIM (0-1 scale): {ssim(pred.unsqueeze(dim=0), original.unsqueeze(dim=0)):.3f} vs Oringal Noisy SSIM: {ssim(noise_sample.unsqueeze(dim=0), original.unsqueeze(dim=0)):.3f}\")   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
